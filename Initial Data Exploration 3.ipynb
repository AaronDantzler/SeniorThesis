{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPT+Ge0JcgujOh1gXOBRPz0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQlCqHTP9N4F","executionInfo":{"status":"ok","timestamp":1734032474780,"user_tz":300,"elapsed":23588,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"985df829-48a2-4903-9bc8-12767629f54d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Autoregressive Linear Regression on just Blood Glucose"],"metadata":{"id":"tqnTDSRvEHjU"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","# Load the data\n","file_path = '/content/drive/My Drive/Thesis/Subject3.xlsx'\n","cgm_data = pd.read_excel(file_path, sheet_name='CGM')"],"metadata":{"id":"ud1OmyUu2v7P","executionInfo":{"status":"ok","timestamp":1734032495136,"user_tz":300,"elapsed":20366,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Preprocess - Round to nearest 5 minutes\n","cgm_data['date'] = pd.to_datetime(cgm_data['date']).dt.round('5min')\n","\n","# Filter out rows where mg/dl is > 400 or missing values\n","cgm_data = cgm_data[cgm_data['mg/dl'] <= 400].dropna(subset=['mg/dl'])\n","\n","# Drop duplicates and sort\n","cgm_data = cgm_data.drop_duplicates(subset='date').reset_index(drop=True)\n","cgm_data = cgm_data.sort_values(by='date').reset_index(drop=True)"],"metadata":{"id":"douYzyaW2xjy","executionInfo":{"status":"ok","timestamp":1734032495138,"user_tz":300,"elapsed":11,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Generate AR Features (using past 12 readings as input)\n","num_lags = 12 # Using the past 12 values\n","for lag in range(1, num_lags + 1):\n","    cgm_data[f'lag_{lag}'] = cgm_data['mg/dl'].shift(lag)\n","\n","cgm_data = cgm_data.dropna().reset_index(drop=True)"],"metadata":{"id":"CRxaWkzv2x6u","executionInfo":{"status":"ok","timestamp":1734032495416,"user_tz":300,"elapsed":287,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Split into train/test (80% train, 20% test by time order)\n","train_size = int(len(cgm_data) * 0.8)\n","train_data = cgm_data[:train_size]\n","test_data = cgm_data[train_size:]"],"metadata":{"id":"OkoX5i1J2002","executionInfo":{"status":"ok","timestamp":1734032495416,"user_tz":300,"elapsed":10,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Define function to train and evaluate for different time steps\n","def train_ar_model(data, target_step):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the model\n","    model = LinearRegression()\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Use np.sqrt on MSE\n","\n","    # Display the coefficients\n","    coefficients = model.coef_\n","    intercept = model.intercept_\n","\n","    print(\"Linear Regression Coefficients (lags):\")\n","    for i, coef in enumerate(coefficients, start=1):\n","      print(f\"Lag {i}: {coef:.4f}\")\n","    print(f\"Intercept: {intercept:.4f}\")\n","\n","    return model, rmse"],"metadata":{"id":"x97E22Mm22Zs","executionInfo":{"status":"ok","timestamp":1734032495417,"user_tz":300,"elapsed":10,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Train models for different time step targets and evaluate\n","time_steps = [3, 6, 9, 12]  # Corresponding to 15, 30, 45, 60 minutes\n","results = {}\n","\n","for step in time_steps:\n","    model, rmse = train_ar_model(cgm_data, step)\n","    results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Time step {step} (predicting {step*5} minutes ahead) - RMSE: {rmse:.4f}\")\n","\n","# Results dictionary now contains trained models and RMSE for each time target"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wp2DlsI24Qc","executionInfo":{"status":"ok","timestamp":1734032495992,"user_tz":300,"elapsed":584,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"54bd94c5-6d6a-49c7-cc6f-b8a37ab38f44"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression Coefficients (lags):\n","Lag 1: 1.4842\n","Lag 2: -0.2540\n","Lag 3: -0.1461\n","Lag 4: -0.0522\n","Lag 5: -0.0158\n","Lag 6: -0.0224\n","Lag 7: -0.0105\n","Lag 8: -0.0244\n","Lag 9: 0.0038\n","Lag 10: -0.0098\n","Lag 11: -0.0030\n","Lag 12: -0.0267\n","Intercept: 14.2951\n","Time step 3 (predicting 15 minutes ahead) - RMSE: 21.7570\n","Linear Regression Coefficients (lags):\n","Lag 1: 1.5623\n","Lag 2: -0.3066\n","Lag 3: -0.1704\n","Lag 4: -0.0736\n","Lag 5: -0.0246\n","Lag 6: -0.0279\n","Lag 7: -0.0206\n","Lag 8: -0.0251\n","Lag 9: -0.0117\n","Lag 10: -0.0123\n","Lag 11: -0.0129\n","Lag 12: -0.0241\n","Intercept: 27.3984\n","Time step 6 (predicting 30 minutes ahead) - RMSE: 32.7853\n","Linear Regression Coefficients (lags):\n","Lag 1: 1.5533\n","Lag 2: -0.3225\n","Lag 3: -0.1781\n","Lag 4: -0.0849\n","Lag 5: -0.0252\n","Lag 6: -0.0446\n","Lag 7: -0.0233\n","Lag 8: -0.0354\n","Lag 9: -0.0098\n","Lag 10: -0.0115\n","Lag 11: -0.0104\n","Lag 12: -0.0277\n","Intercept: 40.9049\n","Time step 9 (predicting 45 minutes ahead) - RMSE: 41.7944\n","Linear Regression Coefficients (lags):\n","Lag 1: 1.4986\n","Lag 2: -0.3176\n","Lag 3: -0.1913\n","Lag 4: -0.0861\n","Lag 5: -0.0355\n","Lag 6: -0.0423\n","Lag 7: -0.0223\n","Lag 8: -0.0317\n","Lag 9: -0.0097\n","Lag 10: -0.0195\n","Lag 11: -0.0183\n","Lag 12: -0.0156\n","Intercept: 54.1536\n","Time step 12 (predicting 60 minutes ahead) - RMSE: 49.0789\n"]}]},{"cell_type":"markdown","source":["ARX Model with Blood Glucose and Bolus Data"],"metadata":{"id":"g3a743rVoLM7"}},{"cell_type":"code","source":["# Load Bolus data\n","bolus_data = pd.read_excel(file_path, sheet_name='Bolus')\n","\n","# Preprocess Bolus data - Round to nearest 5 minutes and fill missing values with 0\n","bolus_data['date'] = pd.to_datetime(bolus_data['date']).dt.round('5min')\n","bolus_data = bolus_data.fillna(0)  # Set missing values to 0\n","\n","# Merge Bolus data with CGM data on date\n","merged_data = pd.merge(cgm_data, bolus_data, on='date', how='left').fillna(0)  # Fill missing values after merge\n","\n","# Generate lag features for Bolus variables\n","bolus_vars = ['normal', 'carbInput', 'insulinCarbRatio', 'bgInput', 'recommended.carb', 'recommended.net',\n","              'recommended.correction', 'insulinSensitivityFactor', 'targetBloodGlucose', 'insulinOnBoard']\n","\n","for var in bolus_vars:\n","    for lag in range(1, num_lags + 1):\n","        merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","\n","# Drop rows with NaNs introduced by shifting\n","merged_data = merged_data.dropna().reset_index(drop=True)\n","\n","# Update train/test split based on merged data\n","train_data = merged_data[:train_size]\n","test_data = merged_data[train_size:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Jt4GA81BcX0","executionInfo":{"status":"ok","timestamp":1734032497748,"user_tz":300,"elapsed":1761,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"292aa1c9-50e8-4066-8355-fa6999dac583"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n","<ipython-input-8-04aca2ebd074>:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  merged_data[f'{var}_lag_{lag}'] = merged_data[var].shift(lag)\n"]}]},{"cell_type":"code","source":["# Modify train_ar_model function to include all lagged features\n","def train_arx_model(data, target_step):\n","    # Prepare inputs (X) - using lag features from both mg/dl and Bolus data\n","    lag_columns = [f'lag_{i}' for i in range(1, num_lags + 1)] + \\\n","                  [f'{var}_lag_{i}' for var in bolus_vars for i in range(1, num_lags + 1)]\n","    X = data[lag_columns].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the model\n","    model = LinearRegression()\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    # Extract coefficients and intercept\n","    coefficients = model.coef_\n","    intercept = model.intercept_\n","\n","    # Display the coefficients\n","    print(\"Linear Regression Coefficients (ARX Model):\")\n","    feature_names = merged_data.drop(columns=['date', 'mg/dl']).columns\n","    for feature, coef in zip(feature_names, coefficients):\n","      print(f\"{feature}: {coef:.4f}\")\n","    print(f\"Intercept: {intercept:.4f}\")\n","\n","    return model, rmse"],"metadata":{"id":"ZAZtpnsdBmZ_","executionInfo":{"status":"ok","timestamp":1734032497749,"user_tz":300,"elapsed":8,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Train ARX models for different time steps and evaluate\n","results_arx = {}\n","for step in time_steps:\n","    model, rmse = train_arx_model(merged_data, step)\n","    results_arx[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Time step {step} (predicting {step*5} minutes ahead) - ARX RMSE: {rmse:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LG4pUQsB1R-","executionInfo":{"status":"ok","timestamp":1734032501709,"user_tz":300,"elapsed":3966,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"c562aa11-f927-4310-ecf0-56c1967ec302"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression Coefficients (ARX Model):\n","lag_1: 1.4842\n","lag_2: -0.2540\n","lag_3: -0.1461\n","lag_4: -0.0522\n","lag_5: -0.0158\n","lag_6: -0.0224\n","lag_7: -0.0105\n","lag_8: -0.0244\n","lag_9: 0.0038\n","lag_10: -0.0098\n","lag_11: -0.0030\n","lag_12: -0.0267\n","normal: 0.0000\n","carbInput: 0.0000\n","insulinCarbRatio: 0.0000\n","bgInput: 0.0000\n","recommended.carb: 0.0000\n","recommended.net: 0.0000\n","recommended.correction: 0.0000\n","insulinSensitivityFactor: 0.0000\n","targetBloodGlucose: 0.0000\n","insulinOnBoard: 0.0000\n","normal_lag_1: 0.0000\n","normal_lag_2: 0.0000\n","normal_lag_3: 0.0000\n","normal_lag_4: 0.0000\n","normal_lag_5: 0.0000\n","normal_lag_6: 0.0000\n","normal_lag_7: 0.0000\n","normal_lag_8: 0.0000\n","normal_lag_9: 0.0000\n","normal_lag_10: 0.0000\n","normal_lag_11: 0.0000\n","normal_lag_12: 0.0000\n","carbInput_lag_1: 0.0000\n","carbInput_lag_2: 0.0000\n","carbInput_lag_3: 0.0000\n","carbInput_lag_4: 0.0000\n","carbInput_lag_5: 0.0000\n","carbInput_lag_6: 0.0000\n","carbInput_lag_7: 0.0000\n","carbInput_lag_8: 0.0000\n","carbInput_lag_9: 0.0000\n","carbInput_lag_10: 0.0000\n","carbInput_lag_11: 0.0000\n","carbInput_lag_12: 0.0000\n","insulinCarbRatio_lag_1: 0.0000\n","insulinCarbRatio_lag_2: 0.0000\n","insulinCarbRatio_lag_3: 0.0000\n","insulinCarbRatio_lag_4: 0.0000\n","insulinCarbRatio_lag_5: 0.0000\n","insulinCarbRatio_lag_6: 0.0000\n","insulinCarbRatio_lag_7: 0.0000\n","insulinCarbRatio_lag_8: 0.0000\n","insulinCarbRatio_lag_9: 0.0000\n","insulinCarbRatio_lag_10: 0.0000\n","insulinCarbRatio_lag_11: 0.0000\n","insulinCarbRatio_lag_12: 0.0000\n","bgInput_lag_1: 0.0000\n","bgInput_lag_2: 0.0000\n","bgInput_lag_3: 0.0000\n","bgInput_lag_4: 0.0000\n","bgInput_lag_5: 0.0000\n","bgInput_lag_6: 0.0000\n","bgInput_lag_7: 0.0000\n","bgInput_lag_8: 0.0000\n","bgInput_lag_9: 0.0000\n","bgInput_lag_10: 0.0000\n","bgInput_lag_11: 0.0000\n","bgInput_lag_12: 0.0000\n","recommended.carb_lag_1: 0.0000\n","recommended.carb_lag_2: 0.0000\n","recommended.carb_lag_3: 0.0000\n","recommended.carb_lag_4: 0.0000\n","recommended.carb_lag_5: 0.0000\n","recommended.carb_lag_6: 0.0000\n","recommended.carb_lag_7: 0.0000\n","recommended.carb_lag_8: 0.0000\n","recommended.carb_lag_9: 0.0000\n","recommended.carb_lag_10: 0.0000\n","recommended.carb_lag_11: 0.0000\n","recommended.carb_lag_12: 0.0000\n","recommended.net_lag_1: 0.0000\n","recommended.net_lag_2: 0.0000\n","recommended.net_lag_3: 0.0000\n","recommended.net_lag_4: 0.0000\n","recommended.net_lag_5: 0.0000\n","recommended.net_lag_6: 0.0000\n","recommended.net_lag_7: 0.0000\n","recommended.net_lag_8: 0.0000\n","recommended.net_lag_9: 0.0000\n","recommended.net_lag_10: 0.0000\n","recommended.net_lag_11: 0.0000\n","recommended.net_lag_12: 0.0000\n","recommended.correction_lag_1: 0.0000\n","recommended.correction_lag_2: 0.0000\n","recommended.correction_lag_3: 0.0000\n","recommended.correction_lag_4: 0.0000\n","recommended.correction_lag_5: 0.0000\n","recommended.correction_lag_6: 0.0000\n","recommended.correction_lag_7: 0.0000\n","recommended.correction_lag_8: 0.0000\n","recommended.correction_lag_9: 0.0000\n","recommended.correction_lag_10: 0.0000\n","recommended.correction_lag_11: 0.0000\n","recommended.correction_lag_12: 0.0000\n","insulinSensitivityFactor_lag_1: 0.0000\n","insulinSensitivityFactor_lag_2: 0.0000\n","insulinSensitivityFactor_lag_3: 0.0000\n","insulinSensitivityFactor_lag_4: 0.0000\n","insulinSensitivityFactor_lag_5: 0.0000\n","insulinSensitivityFactor_lag_6: 0.0000\n","insulinSensitivityFactor_lag_7: 0.0000\n","insulinSensitivityFactor_lag_8: 0.0000\n","insulinSensitivityFactor_lag_9: 0.0000\n","insulinSensitivityFactor_lag_10: 0.0000\n","insulinSensitivityFactor_lag_11: 0.0000\n","insulinSensitivityFactor_lag_12: 0.0000\n","targetBloodGlucose_lag_1: 0.0000\n","targetBloodGlucose_lag_2: 0.0000\n","targetBloodGlucose_lag_3: 0.0000\n","targetBloodGlucose_lag_4: 0.0000\n","targetBloodGlucose_lag_5: 0.0000\n","targetBloodGlucose_lag_6: 0.0000\n","targetBloodGlucose_lag_7: 0.0000\n","targetBloodGlucose_lag_8: 0.0000\n","targetBloodGlucose_lag_9: 0.0000\n","targetBloodGlucose_lag_10: 0.0000\n","targetBloodGlucose_lag_11: 0.0000\n","targetBloodGlucose_lag_12: 0.0000\n","insulinOnBoard_lag_1: 0.0000\n","insulinOnBoard_lag_2: 0.0000\n","Intercept: 14.2953\n","Time step 3 (predicting 15 minutes ahead) - ARX RMSE: 21.7582\n","Linear Regression Coefficients (ARX Model):\n","lag_1: 1.5623\n","lag_2: -0.3066\n","lag_3: -0.1704\n","lag_4: -0.0736\n","lag_5: -0.0246\n","lag_6: -0.0279\n","lag_7: -0.0206\n","lag_8: -0.0251\n","lag_9: -0.0117\n","lag_10: -0.0123\n","lag_11: -0.0129\n","lag_12: -0.0241\n","normal: 0.0000\n","carbInput: 0.0000\n","insulinCarbRatio: 0.0000\n","bgInput: 0.0000\n","recommended.carb: 0.0000\n","recommended.net: 0.0000\n","recommended.correction: 0.0000\n","insulinSensitivityFactor: 0.0000\n","targetBloodGlucose: 0.0000\n","insulinOnBoard: 0.0000\n","normal_lag_1: 0.0000\n","normal_lag_2: 0.0000\n","normal_lag_3: 0.0000\n","normal_lag_4: 0.0000\n","normal_lag_5: 0.0000\n","normal_lag_6: 0.0000\n","normal_lag_7: 0.0000\n","normal_lag_8: 0.0000\n","normal_lag_9: 0.0000\n","normal_lag_10: 0.0000\n","normal_lag_11: 0.0000\n","normal_lag_12: 0.0000\n","carbInput_lag_1: 0.0000\n","carbInput_lag_2: 0.0000\n","carbInput_lag_3: 0.0000\n","carbInput_lag_4: 0.0000\n","carbInput_lag_5: 0.0000\n","carbInput_lag_6: 0.0000\n","carbInput_lag_7: 0.0000\n","carbInput_lag_8: 0.0000\n","carbInput_lag_9: 0.0000\n","carbInput_lag_10: 0.0000\n","carbInput_lag_11: 0.0000\n","carbInput_lag_12: 0.0000\n","insulinCarbRatio_lag_1: 0.0000\n","insulinCarbRatio_lag_2: 0.0000\n","insulinCarbRatio_lag_3: 0.0000\n","insulinCarbRatio_lag_4: 0.0000\n","insulinCarbRatio_lag_5: 0.0000\n","insulinCarbRatio_lag_6: 0.0000\n","insulinCarbRatio_lag_7: 0.0000\n","insulinCarbRatio_lag_8: 0.0000\n","insulinCarbRatio_lag_9: 0.0000\n","insulinCarbRatio_lag_10: 0.0000\n","insulinCarbRatio_lag_11: 0.0000\n","insulinCarbRatio_lag_12: 0.0000\n","bgInput_lag_1: 0.0000\n","bgInput_lag_2: 0.0000\n","bgInput_lag_3: 0.0000\n","bgInput_lag_4: 0.0000\n","bgInput_lag_5: 0.0000\n","bgInput_lag_6: 0.0000\n","bgInput_lag_7: 0.0000\n","bgInput_lag_8: 0.0000\n","bgInput_lag_9: 0.0000\n","bgInput_lag_10: 0.0000\n","bgInput_lag_11: 0.0000\n","bgInput_lag_12: 0.0000\n","recommended.carb_lag_1: 0.0000\n","recommended.carb_lag_2: 0.0000\n","recommended.carb_lag_3: 0.0000\n","recommended.carb_lag_4: 0.0000\n","recommended.carb_lag_5: 0.0000\n","recommended.carb_lag_6: 0.0000\n","recommended.carb_lag_7: 0.0000\n","recommended.carb_lag_8: 0.0000\n","recommended.carb_lag_9: 0.0000\n","recommended.carb_lag_10: 0.0000\n","recommended.carb_lag_11: 0.0000\n","recommended.carb_lag_12: 0.0000\n","recommended.net_lag_1: 0.0000\n","recommended.net_lag_2: 0.0000\n","recommended.net_lag_3: 0.0000\n","recommended.net_lag_4: 0.0000\n","recommended.net_lag_5: 0.0000\n","recommended.net_lag_6: 0.0000\n","recommended.net_lag_7: 0.0000\n","recommended.net_lag_8: 0.0000\n","recommended.net_lag_9: 0.0000\n","recommended.net_lag_10: 0.0000\n","recommended.net_lag_11: 0.0000\n","recommended.net_lag_12: 0.0000\n","recommended.correction_lag_1: 0.0000\n","recommended.correction_lag_2: 0.0000\n","recommended.correction_lag_3: 0.0000\n","recommended.correction_lag_4: 0.0000\n","recommended.correction_lag_5: 0.0000\n","recommended.correction_lag_6: 0.0000\n","recommended.correction_lag_7: 0.0000\n","recommended.correction_lag_8: 0.0000\n","recommended.correction_lag_9: 0.0000\n","recommended.correction_lag_10: 0.0000\n","recommended.correction_lag_11: 0.0000\n","recommended.correction_lag_12: 0.0000\n","insulinSensitivityFactor_lag_1: 0.0000\n","insulinSensitivityFactor_lag_2: 0.0000\n","insulinSensitivityFactor_lag_3: 0.0000\n","insulinSensitivityFactor_lag_4: 0.0000\n","insulinSensitivityFactor_lag_5: 0.0000\n","insulinSensitivityFactor_lag_6: 0.0000\n","insulinSensitivityFactor_lag_7: 0.0000\n","insulinSensitivityFactor_lag_8: 0.0000\n","insulinSensitivityFactor_lag_9: 0.0000\n","insulinSensitivityFactor_lag_10: 0.0000\n","insulinSensitivityFactor_lag_11: 0.0000\n","insulinSensitivityFactor_lag_12: 0.0000\n","targetBloodGlucose_lag_1: 0.0000\n","targetBloodGlucose_lag_2: 0.0000\n","targetBloodGlucose_lag_3: 0.0000\n","targetBloodGlucose_lag_4: 0.0000\n","targetBloodGlucose_lag_5: 0.0000\n","targetBloodGlucose_lag_6: 0.0000\n","targetBloodGlucose_lag_7: 0.0000\n","targetBloodGlucose_lag_8: 0.0000\n","targetBloodGlucose_lag_9: 0.0000\n","targetBloodGlucose_lag_10: 0.0000\n","targetBloodGlucose_lag_11: 0.0000\n","targetBloodGlucose_lag_12: 0.0000\n","insulinOnBoard_lag_1: 0.0000\n","insulinOnBoard_lag_2: 0.0000\n","Intercept: 27.3985\n","Time step 6 (predicting 30 minutes ahead) - ARX RMSE: 32.7876\n","Linear Regression Coefficients (ARX Model):\n","lag_1: 1.5533\n","lag_2: -0.3225\n","lag_3: -0.1781\n","lag_4: -0.0849\n","lag_5: -0.0252\n","lag_6: -0.0446\n","lag_7: -0.0233\n","lag_8: -0.0354\n","lag_9: -0.0098\n","lag_10: -0.0115\n","lag_11: -0.0103\n","lag_12: -0.0277\n","normal: 0.0000\n","carbInput: 0.0000\n","insulinCarbRatio: 0.0000\n","bgInput: 0.0000\n","recommended.carb: 0.0000\n","recommended.net: 0.0000\n","recommended.correction: 0.0000\n","insulinSensitivityFactor: 0.0000\n","targetBloodGlucose: 0.0000\n","insulinOnBoard: 0.0000\n","normal_lag_1: 0.0000\n","normal_lag_2: 0.0000\n","normal_lag_3: 0.0000\n","normal_lag_4: 0.0000\n","normal_lag_5: 0.0000\n","normal_lag_6: 0.0000\n","normal_lag_7: 0.0000\n","normal_lag_8: 0.0000\n","normal_lag_9: 0.0000\n","normal_lag_10: 0.0000\n","normal_lag_11: 0.0000\n","normal_lag_12: 0.0000\n","carbInput_lag_1: 0.0000\n","carbInput_lag_2: 0.0000\n","carbInput_lag_3: 0.0000\n","carbInput_lag_4: 0.0000\n","carbInput_lag_5: 0.0000\n","carbInput_lag_6: 0.0000\n","carbInput_lag_7: 0.0000\n","carbInput_lag_8: 0.0000\n","carbInput_lag_9: 0.0000\n","carbInput_lag_10: 0.0000\n","carbInput_lag_11: 0.0000\n","carbInput_lag_12: 0.0000\n","insulinCarbRatio_lag_1: 0.0000\n","insulinCarbRatio_lag_2: 0.0000\n","insulinCarbRatio_lag_3: 0.0000\n","insulinCarbRatio_lag_4: 0.0000\n","insulinCarbRatio_lag_5: 0.0000\n","insulinCarbRatio_lag_6: 0.0000\n","insulinCarbRatio_lag_7: 0.0000\n","insulinCarbRatio_lag_8: 0.0000\n","insulinCarbRatio_lag_9: 0.0000\n","insulinCarbRatio_lag_10: 0.0000\n","insulinCarbRatio_lag_11: 0.0000\n","insulinCarbRatio_lag_12: 0.0000\n","bgInput_lag_1: 0.0000\n","bgInput_lag_2: 0.0000\n","bgInput_lag_3: 0.0000\n","bgInput_lag_4: 0.0000\n","bgInput_lag_5: 0.0000\n","bgInput_lag_6: 0.0000\n","bgInput_lag_7: 0.0000\n","bgInput_lag_8: 0.0000\n","bgInput_lag_9: 0.0000\n","bgInput_lag_10: 0.0000\n","bgInput_lag_11: 0.0000\n","bgInput_lag_12: 0.0000\n","recommended.carb_lag_1: 0.0000\n","recommended.carb_lag_2: 0.0000\n","recommended.carb_lag_3: 0.0000\n","recommended.carb_lag_4: 0.0000\n","recommended.carb_lag_5: 0.0000\n","recommended.carb_lag_6: 0.0000\n","recommended.carb_lag_7: 0.0000\n","recommended.carb_lag_8: 0.0000\n","recommended.carb_lag_9: 0.0000\n","recommended.carb_lag_10: 0.0000\n","recommended.carb_lag_11: 0.0000\n","recommended.carb_lag_12: 0.0000\n","recommended.net_lag_1: 0.0000\n","recommended.net_lag_2: 0.0000\n","recommended.net_lag_3: 0.0000\n","recommended.net_lag_4: 0.0000\n","recommended.net_lag_5: 0.0000\n","recommended.net_lag_6: 0.0000\n","recommended.net_lag_7: 0.0000\n","recommended.net_lag_8: 0.0000\n","recommended.net_lag_9: 0.0000\n","recommended.net_lag_10: 0.0000\n","recommended.net_lag_11: 0.0000\n","recommended.net_lag_12: 0.0000\n","recommended.correction_lag_1: 0.0000\n","recommended.correction_lag_2: 0.0000\n","recommended.correction_lag_3: 0.0000\n","recommended.correction_lag_4: 0.0000\n","recommended.correction_lag_5: 0.0000\n","recommended.correction_lag_6: 0.0000\n","recommended.correction_lag_7: 0.0000\n","recommended.correction_lag_8: 0.0000\n","recommended.correction_lag_9: 0.0000\n","recommended.correction_lag_10: 0.0000\n","recommended.correction_lag_11: 0.0000\n","recommended.correction_lag_12: 0.0000\n","insulinSensitivityFactor_lag_1: 0.0000\n","insulinSensitivityFactor_lag_2: 0.0000\n","insulinSensitivityFactor_lag_3: 0.0000\n","insulinSensitivityFactor_lag_4: 0.0000\n","insulinSensitivityFactor_lag_5: 0.0000\n","insulinSensitivityFactor_lag_6: 0.0000\n","insulinSensitivityFactor_lag_7: 0.0000\n","insulinSensitivityFactor_lag_8: 0.0000\n","insulinSensitivityFactor_lag_9: 0.0000\n","insulinSensitivityFactor_lag_10: 0.0000\n","insulinSensitivityFactor_lag_11: 0.0000\n","insulinSensitivityFactor_lag_12: 0.0000\n","targetBloodGlucose_lag_1: 0.0000\n","targetBloodGlucose_lag_2: 0.0000\n","targetBloodGlucose_lag_3: 0.0000\n","targetBloodGlucose_lag_4: 0.0000\n","targetBloodGlucose_lag_5: 0.0000\n","targetBloodGlucose_lag_6: 0.0000\n","targetBloodGlucose_lag_7: 0.0000\n","targetBloodGlucose_lag_8: 0.0000\n","targetBloodGlucose_lag_9: 0.0000\n","targetBloodGlucose_lag_10: 0.0000\n","targetBloodGlucose_lag_11: 0.0000\n","targetBloodGlucose_lag_12: 0.0000\n","insulinOnBoard_lag_1: 0.0000\n","insulinOnBoard_lag_2: 0.0000\n","Intercept: 40.9052\n","Time step 9 (predicting 45 minutes ahead) - ARX RMSE: 41.7972\n","Linear Regression Coefficients (ARX Model):\n","lag_1: 1.4986\n","lag_2: -0.3176\n","lag_3: -0.1913\n","lag_4: -0.0861\n","lag_5: -0.0355\n","lag_6: -0.0423\n","lag_7: -0.0223\n","lag_8: -0.0317\n","lag_9: -0.0097\n","lag_10: -0.0195\n","lag_11: -0.0182\n","lag_12: -0.0156\n","normal: 0.0000\n","carbInput: 0.0000\n","insulinCarbRatio: 0.0000\n","bgInput: 0.0000\n","recommended.carb: 0.0000\n","recommended.net: 0.0000\n","recommended.correction: 0.0000\n","insulinSensitivityFactor: 0.0000\n","targetBloodGlucose: 0.0000\n","insulinOnBoard: 0.0000\n","normal_lag_1: 0.0000\n","normal_lag_2: 0.0000\n","normal_lag_3: 0.0000\n","normal_lag_4: 0.0000\n","normal_lag_5: 0.0000\n","normal_lag_6: 0.0000\n","normal_lag_7: 0.0000\n","normal_lag_8: 0.0000\n","normal_lag_9: 0.0000\n","normal_lag_10: 0.0000\n","normal_lag_11: 0.0000\n","normal_lag_12: 0.0000\n","carbInput_lag_1: 0.0000\n","carbInput_lag_2: 0.0000\n","carbInput_lag_3: 0.0000\n","carbInput_lag_4: 0.0000\n","carbInput_lag_5: 0.0000\n","carbInput_lag_6: 0.0000\n","carbInput_lag_7: 0.0000\n","carbInput_lag_8: 0.0000\n","carbInput_lag_9: 0.0000\n","carbInput_lag_10: 0.0000\n","carbInput_lag_11: 0.0000\n","carbInput_lag_12: 0.0000\n","insulinCarbRatio_lag_1: 0.0000\n","insulinCarbRatio_lag_2: 0.0000\n","insulinCarbRatio_lag_3: 0.0000\n","insulinCarbRatio_lag_4: 0.0000\n","insulinCarbRatio_lag_5: 0.0000\n","insulinCarbRatio_lag_6: 0.0000\n","insulinCarbRatio_lag_7: 0.0000\n","insulinCarbRatio_lag_8: 0.0000\n","insulinCarbRatio_lag_9: 0.0000\n","insulinCarbRatio_lag_10: 0.0000\n","insulinCarbRatio_lag_11: 0.0000\n","insulinCarbRatio_lag_12: 0.0000\n","bgInput_lag_1: 0.0000\n","bgInput_lag_2: 0.0000\n","bgInput_lag_3: 0.0000\n","bgInput_lag_4: 0.0000\n","bgInput_lag_5: 0.0000\n","bgInput_lag_6: 0.0000\n","bgInput_lag_7: 0.0000\n","bgInput_lag_8: 0.0000\n","bgInput_lag_9: 0.0000\n","bgInput_lag_10: 0.0000\n","bgInput_lag_11: 0.0000\n","bgInput_lag_12: 0.0000\n","recommended.carb_lag_1: 0.0000\n","recommended.carb_lag_2: 0.0000\n","recommended.carb_lag_3: 0.0000\n","recommended.carb_lag_4: 0.0000\n","recommended.carb_lag_5: 0.0000\n","recommended.carb_lag_6: 0.0000\n","recommended.carb_lag_7: 0.0000\n","recommended.carb_lag_8: 0.0000\n","recommended.carb_lag_9: 0.0000\n","recommended.carb_lag_10: 0.0000\n","recommended.carb_lag_11: 0.0000\n","recommended.carb_lag_12: 0.0000\n","recommended.net_lag_1: 0.0000\n","recommended.net_lag_2: 0.0000\n","recommended.net_lag_3: 0.0000\n","recommended.net_lag_4: 0.0000\n","recommended.net_lag_5: 0.0000\n","recommended.net_lag_6: 0.0000\n","recommended.net_lag_7: 0.0000\n","recommended.net_lag_8: 0.0000\n","recommended.net_lag_9: 0.0000\n","recommended.net_lag_10: 0.0000\n","recommended.net_lag_11: 0.0000\n","recommended.net_lag_12: 0.0000\n","recommended.correction_lag_1: 0.0000\n","recommended.correction_lag_2: 0.0000\n","recommended.correction_lag_3: 0.0000\n","recommended.correction_lag_4: 0.0000\n","recommended.correction_lag_5: 0.0000\n","recommended.correction_lag_6: 0.0000\n","recommended.correction_lag_7: 0.0000\n","recommended.correction_lag_8: 0.0000\n","recommended.correction_lag_9: 0.0000\n","recommended.correction_lag_10: 0.0000\n","recommended.correction_lag_11: 0.0000\n","recommended.correction_lag_12: 0.0000\n","insulinSensitivityFactor_lag_1: 0.0000\n","insulinSensitivityFactor_lag_2: 0.0000\n","insulinSensitivityFactor_lag_3: 0.0000\n","insulinSensitivityFactor_lag_4: 0.0000\n","insulinSensitivityFactor_lag_5: 0.0000\n","insulinSensitivityFactor_lag_6: 0.0000\n","insulinSensitivityFactor_lag_7: 0.0000\n","insulinSensitivityFactor_lag_8: 0.0000\n","insulinSensitivityFactor_lag_9: 0.0000\n","insulinSensitivityFactor_lag_10: 0.0000\n","insulinSensitivityFactor_lag_11: 0.0000\n","insulinSensitivityFactor_lag_12: 0.0000\n","targetBloodGlucose_lag_1: 0.0000\n","targetBloodGlucose_lag_2: 0.0000\n","targetBloodGlucose_lag_3: 0.0000\n","targetBloodGlucose_lag_4: 0.0000\n","targetBloodGlucose_lag_5: 0.0000\n","targetBloodGlucose_lag_6: 0.0000\n","targetBloodGlucose_lag_7: 0.0000\n","targetBloodGlucose_lag_8: 0.0000\n","targetBloodGlucose_lag_9: 0.0000\n","targetBloodGlucose_lag_10: 0.0000\n","targetBloodGlucose_lag_11: 0.0000\n","targetBloodGlucose_lag_12: 0.0000\n","insulinOnBoard_lag_1: 0.0000\n","insulinOnBoard_lag_2: 0.0000\n","Intercept: 54.1542\n","Time step 12 (predicting 60 minutes ahead) - ARX RMSE: 49.0821\n"]}]},{"cell_type":"markdown","source":["Recursive AR Model"],"metadata":{"id":"il6JtroVCShw"}},{"cell_type":"code","source":["# Recursive forecasting function for the AR model\n","def recursive_forecast_ar_model(data, model, num_steps):\n","    # Initialize a list to store predictions\n","    predictions = []\n","\n","    # Get the last 'num_lags' lag values from the data for initial input\n","    last_known_data = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].iloc[-1].values\n","\n","    # Generate recursive predictions\n","    for _ in range(num_steps):\n","        # Reshape last known data to fit model input format\n","        next_input = last_known_data.reshape(1, -1)\n","\n","        # Predict the next step\n","        next_pred = model.predict(next_input)[0]\n","        predictions.append(next_pred)\n","\n","        # Update the input data by adding the new prediction and dropping the oldest value\n","        last_known_data = np.roll(last_known_data, -1)  # Shift values left\n","        last_known_data[-1] = next_pred  # Add new prediction as the latest lag\n","\n","    return predictions"],"metadata":{"id":"HkIlwTAcIGPg","executionInfo":{"status":"ok","timestamp":1734032501710,"user_tz":300,"elapsed":11,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Function to train the AR model on 1-step-ahead predictions\n","def train_single_step_ar_model(data):\n","    # Prepare inputs (X) and target (y) for single step\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-1).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the model\n","    model = LinearRegression()\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set (1-step-ahead for evaluation)\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return model, rmse"],"metadata":{"id":"sHM5bCFsIHft","executionInfo":{"status":"ok","timestamp":1734032501710,"user_tz":300,"elapsed":9,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Train the single-step AR model and evaluate\n","single_step_model, single_step_rmse = train_single_step_ar_model(cgm_data)\n","print(f\"Single-step prediction model - RMSE: {single_step_rmse:.4f}\")\n","\n","# Perform recursive forecasting for different time steps\n","future_steps = [3, 6, 9, 12]  # 15, 30, 45, 60 minutes ahead\n","recursive_results = {}\n","\n","for step in future_steps:\n","    # Recursive prediction\n","    predictions = recursive_forecast_ar_model(cgm_data, single_step_model, step)\n","    recursive_results[f'{step*5} min ahead'] = predictions\n","    print(f\"Recursive prediction for {step*5} minutes ahead: {predictions}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ji0Xv3QuIPyE","executionInfo":{"status":"ok","timestamp":1734032501831,"user_tz":300,"elapsed":129,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"eeab7600-dd6a-417e-9384-a4d388c5ada9"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Single-step prediction model - RMSE: 12.5982\n","Recursive prediction for 15 minutes ahead: [339.6928118539782, 339.51801954064416, 348.0935274339459]\n","Recursive prediction for 30 minutes ahead: [339.6928118539782, 339.51801954064416, 348.0935274339459, 350.08779219833457, 359.0715724859542, 357.14252184751274]\n","Recursive prediction for 45 minutes ahead: [339.6928118539782, 339.51801954064416, 348.0935274339459, 350.08779219833457, 359.0715724859542, 357.14252184751274, 350.7281561698034, 352.7432479005379, 352.14976987336934]\n","Recursive prediction for 60 minutes ahead: [339.6928118539782, 339.51801954064416, 348.0935274339459, 350.08779219833457, 359.0715724859542, 357.14252184751274, 350.7281561698034, 352.7432479005379, 352.14976987336934, 345.8140416478463, 350.2752922701164, 344.6496848665017]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","\n","# Function to calculate RMSE for each recursive forecast\n","def calculate_recursive_rmse(data, model, num_steps):\n","    # Initialize list to store RMSE values\n","    rmse_values = {}\n","\n","    # Iterate through each time step to calculate RMSE\n","    for step in num_steps:\n","        # Get the recursive predictions for the current step\n","        predictions = recursive_forecast_ar_model(data, model, step)\n","\n","        # Get the actual values corresponding to this step\n","        actual_values = data['mg/dl'].shift(-step).dropna().values[:len(predictions)]\n","\n","        # Calculate RMSE for this time step\n","        rmse = np.sqrt(mean_squared_error(actual_values, predictions))\n","        rmse_values[f'{step*5} min ahead'] = rmse\n","        print(f\"RMSE for recursive forecast {step*5} minutes ahead: {rmse:.4f}\")\n","\n","    return rmse_values\n","\n","# Calculate RMSE for recursive forecasts on the test set\n","recursive_rmse = calculate_recursive_rmse(cgm_data[train_size:], single_step_model, future_steps)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlBYNsjVInan","executionInfo":{"status":"ok","timestamp":1734032501832,"user_tz":300,"elapsed":13,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"e720c41c-3f34-48a2-dfac-d5b610a3f55c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE for recursive forecast 15 minutes ahead: 175.1393\n","RMSE for recursive forecast 30 minutes ahead: 179.3985\n","RMSE for recursive forecast 45 minutes ahead: 175.0522\n","RMSE for recursive forecast 60 minutes ahead: 166.2287\n"]}]},{"cell_type":"markdown","source":["LASSO Regression"],"metadata":{"id":"NkjsoBRYIvw9"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","\n","# Function to train and evaluate a Lasso regression model for a specified prediction step\n","def train_lasso_model(data, target_step, alpha=0.1, max_iter=5000):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the Lasso model with specified alpha (regularization strength)\n","    lasso_model = Lasso(alpha=alpha, max_iter=max_iter)\n","    lasso_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = lasso_model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return lasso_model, rmse"],"metadata":{"id":"W0vXtL8pM4Op","executionInfo":{"status":"ok","timestamp":1734032501832,"user_tz":300,"elapsed":8,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Train and evaluate Lasso models for different time steps\n","lasso_results = {}\n","for step in time_steps:\n","    model, rmse = train_lasso_model(cgm_data, step, alpha=0.1, max_iter=5000)\n","    lasso_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Lasso regression for {step*5} minutes ahead - RMSE: {rmse:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btYD6roIM5XK","executionInfo":{"status":"ok","timestamp":1734032516418,"user_tz":300,"elapsed":14593,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"96705132-6e89-4256-823f-a5d9d5c0966c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Lasso regression for 15 minutes ahead - RMSE: 21.7597\n","Lasso regression for 30 minutes ahead - RMSE: 32.7875\n","Lasso regression for 45 minutes ahead - RMSE: 41.7964\n","Lasso regression for 60 minutes ahead - RMSE: 49.0805\n"]}]},{"cell_type":"markdown","source":["Ridge Regression"],"metadata":{"id":"3jrB_xynPg3_"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","\n","# Function to train and evaluate a Ridge regression model for a specified prediction step\n","def train_ridge_model(data, target_step, alpha=1.0):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the Ridge model with specified alpha (regularization strength)\n","    ridge_model = Ridge(alpha=alpha)\n","    ridge_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = ridge_model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return ridge_model, rmse\n","\n","# Train and evaluate Ridge models for different time steps\n","ridge_results = {}\n","alpha_value = 1.0  # Adjust alpha as needed for regularization strength\n","for step in time_steps:\n","    model, rmse = train_ridge_model(cgm_data, step, alpha=alpha_value)\n","    ridge_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Ridge regression for {step*5} minutes ahead - RMSE: {rmse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyVdNSkKSL5j","executionInfo":{"status":"ok","timestamp":1734032516534,"user_tz":300,"elapsed":121,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"c57aead7-3c8e-434b-a81d-5abc3de77648"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Ridge regression for 15 minutes ahead - RMSE: 21.7570\n","Ridge regression for 30 minutes ahead - RMSE: 32.7853\n","Ridge regression for 45 minutes ahead - RMSE: 41.7944\n","Ridge regression for 60 minutes ahead - RMSE: 49.0789\n"]}]},{"cell_type":"markdown","source":["Elastic Net Regression"],"metadata":{"id":"l7xmEKZHYzVa"}},{"cell_type":"code","source":["from sklearn.linear_model import ElasticNet\n","\n","# Function to train and evaluate ElasticNet model\n","def train_elastic_net_model(data, target_step, alpha=1.0, l1_ratio=0.5):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the ElasticNet model\n","    elastic_net_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n","    elastic_net_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = elastic_net_model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return elastic_net_model, rmse\n","\n","# Train and evaluate ElasticNet models for different time steps\n","elastic_net_results = {}\n","alpha_value = 1.0  # Regularization strength (larger values => more regularization)\n","l1_ratio_value = 0.5  # Mix ratio between Lasso (L1) and Ridge (L2) regularization\n","for step in time_steps:\n","    model, rmse = train_elastic_net_model(cgm_data, step, alpha=alpha_value, l1_ratio=l1_ratio_value)\n","    elastic_net_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"ElasticNet for {step*5} minutes ahead - RMSE: {rmse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xelNG6-fY-hH","executionInfo":{"status":"ok","timestamp":1734032526925,"user_tz":300,"elapsed":10395,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"e282427c-d055-46e9-9bf6-804db0f2ab45"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["ElasticNet for 15 minutes ahead - RMSE: 21.7821\n","ElasticNet for 30 minutes ahead - RMSE: 32.8068\n","ElasticNet for 45 minutes ahead - RMSE: 41.8125\n","ElasticNet for 60 minutes ahead - RMSE: 49.0940\n"]}]},{"cell_type":"markdown","source":["Huber"],"metadata":{"id":"dElBbo6IZGGM"}},{"cell_type":"code","source":["from sklearn.linear_model import HuberRegressor\n","\n","# Function to train and evaluate Huber regression model\n","def train_huber_model(data, target_step, epsilon=1.35):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the Huber regression model\n","    huber_model = HuberRegressor(epsilon=epsilon, max_iter=1000)\n","    huber_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = huber_model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return huber_model, rmse\n","\n","# Train and evaluate Huber models for different time steps\n","huber_results = {}\n","epsilon_value = 1.35  # Huber loss function parameter (controls the threshold between quadratic and linear loss)\n","for step in time_steps:\n","    model, rmse = train_huber_model(cgm_data, step, epsilon=epsilon_value)\n","    huber_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Huber for {step*5} minutes ahead - RMSE: {rmse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaMt0glzZLSK","executionInfo":{"status":"ok","timestamp":1734032604795,"user_tz":300,"elapsed":77876,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"5a343d0f-785e-42a1-98bc-e9fed5a6380d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Huber for 15 minutes ahead - RMSE: 21.6874\n","Huber for 30 minutes ahead - RMSE: 32.9093\n","Huber for 45 minutes ahead - RMSE: 42.1399\n","Huber for 60 minutes ahead - RMSE: 49.5920\n"]}]},{"cell_type":"markdown","source":["Random Forest"],"metadata":{"id":"_GyZIr2Ca_Mo"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Function to train and evaluate Random Forest model\n","def train_rf_model(data, target_step, n_estimators=100, max_depth=None, random_state=42):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Train the Random Forest model\n","    rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n","    rf_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = rf_model.predict(X_test)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return rf_model, rmse\n","\n","# Train and evaluate Random Forest models for different time steps\n","rf_results = {}\n","for step in time_steps:\n","    model, rmse = train_rf_model(cgm_data, step, n_estimators=100, max_depth=10)\n","    rf_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"Random Forest for {step*5} minutes ahead - RMSE: {rmse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccFkUnHchZzE","executionInfo":{"status":"ok","timestamp":1734033108210,"user_tz":300,"elapsed":503450,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"7a11e90b-6ca3-46d9-faf6-b6cc81ab9529"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest for 15 minutes ahead - RMSE: 20.8668\n","Random Forest for 30 minutes ahead - RMSE: 31.6224\n","Random Forest for 45 minutes ahead - RMSE: 40.5295\n","Random Forest for 60 minutes ahead - RMSE: 47.8247\n"]}]},{"cell_type":"markdown","source":["XGBoost"],"metadata":{"id":"opeXoGyzbAGT"}},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","\n","# Function to train and evaluate XGBoost model\n","def train_xgb_model(data, target_step, num_boost_round=100, learning_rate=0.1, max_depth=30, random_state=42):\n","    # Prepare inputs (X) and target (y)\n","    X = data[[f'lag_{i}' for i in range(1, num_lags + 1)]].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Create DMatrix for XGBoost\n","    dtrain = xgb.DMatrix(X_train, label=y_train)\n","    dtest = xgb.DMatrix(X_test, label=y_test)\n","\n","    # Set parameters for XGBoost\n","    params = {\n","        'objective': 'reg:squarederror',  # Regression task (squared error loss)\n","        'eval_metric': 'rmse',            # Metric to evaluate model\n","        'max_depth': max_depth,           # Depth of the trees\n","        'learning_rate': learning_rate,  # Step size shrinkage\n","        'random_state': random_state,    # For reproducibility\n","    }\n","\n","    # Train the XGBoost model\n","    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n","\n","    # Make predictions\n","    y_pred = model.predict(dtest)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return model, rmse\n","\n","# Train and evaluate XGBoost models for different time steps\n","xgb_results = {}\n","for step in time_steps:\n","    model, rmse = train_xgb_model(cgm_data, step, num_boost_round=100, learning_rate=0.1, max_depth=10)\n","    xgb_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"XGBoost for {step*5} minutes ahead - RMSE: {rmse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFksTsoXoejo","executionInfo":{"status":"ok","timestamp":1734033128300,"user_tz":300,"elapsed":20115,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"e62b668a-478c-415c-d823-ad9483b36f50"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost for 15 minutes ahead - RMSE: 20.8448\n","XGBoost for 30 minutes ahead - RMSE: 31.5458\n","XGBoost for 45 minutes ahead - RMSE: 40.4536\n","XGBoost for 60 minutes ahead - RMSE: 47.7964\n"]}]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Function to train and evaluate XGBoost model with ARX features\n","def train_xgb_arx_model(data, target_step, num_boost_round=100, learning_rate=0.1, max_depth=10, random_state=42):\n","    # Prepare inputs (X) and target (y)\n","    # Use all lagged features (CGM + Bolus variables)\n","    feature_columns = [col for col in data.columns if col.startswith('lag_') or '_lag_' in col]\n","    X = data[feature_columns].values\n","    y = data['mg/dl'].shift(-target_step).dropna().values\n","    X = X[:len(y)]  # Align lengths\n","\n","    # Split into train/test\n","    X_train, X_test = X[:train_size], X[train_size:]\n","    y_train, y_test = y[:train_size], y[train_size:]\n","\n","    # Create DMatrix for XGBoost\n","    dtrain = xgb.DMatrix(X_train, label=y_train)\n","    dtest = xgb.DMatrix(X_test, label=y_test)\n","\n","    # Set parameters for XGBoost\n","    params = {\n","        'objective': 'reg:squarederror',  # Regression task (squared error loss)\n","        'eval_metric': 'rmse',            # Metric to evaluate model\n","        'max_depth': max_depth,           # Depth of the trees\n","        'learning_rate': learning_rate,   # Step size shrinkage\n","        'random_state': random_state,     # For reproducibility\n","    }\n","\n","    # Train the XGBoost model\n","    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n","\n","    # Make predictions\n","    y_pred = model.predict(dtest)\n","\n","    # Calculate performance metric (RMSE)\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","    return model, rmse\n","\n","# Train and evaluate XGBoost models for different time steps\n","xgb_arx_results = {}\n","for step in time_steps:\n","    model, rmse = train_xgb_arx_model(merged_data, step, num_boost_round=100, learning_rate=0.1, max_depth=10)\n","    xgb_arx_results[f'{step*5} min ahead'] = {'Model': model, 'RMSE': rmse}\n","    print(f\"XGBoost (ARX) for {step*5} minutes ahead - RMSE: {rmse:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rX7CK7TBFPM","executionInfo":{"status":"ok","timestamp":1734033180885,"user_tz":300,"elapsed":52595,"user":{"displayName":"Aaron Dantzler","userId":"01397289172024144959"}},"outputId":"6ea47d0e-78f2-413d-dc16-b17adf7d2890"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost (ARX) for 15 minutes ahead - RMSE: 20.8758\n","XGBoost (ARX) for 30 minutes ahead - RMSE: 31.5396\n","XGBoost (ARX) for 45 minutes ahead - RMSE: 40.4618\n","XGBoost (ARX) for 60 minutes ahead - RMSE: 47.8178\n"]}]}]}